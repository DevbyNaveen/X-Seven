"""AI Conversation Handler

Coordinates intent detection, DB lookups, retrieval, and LLM (with graceful
fallback) to produce helpful, markdown-formatted chat responses.

This module is written to be self-contained and resilient:
- Works without API keys (falls back to heuristic responses)
- Uses DB search first; vector/RAG can be added later or enabled when ready
- Persists messages only when a concrete business_id is available
- Includes character-by-character streaming for typewriter effect
"""
from __future__ import annotations

from typing import Any, Dict, List, Optional, Tuple, AsyncGenerator
import json
from dataclasses import dataclass
import re
import os
import time
import asyncio
from datetime import datetime, timedelta

from sqlalchemy.orm import Session
from sqlalchemy import or_

from app.models import Business, MenuItem, Message
from app.config.settings import settings


# -------- Helpers --------

def _now_ms() -> int:
    return int(time.time() * 1000)


def _md_list(items: List[str]) -> str:
    return "\n".join([f"- {it}" for it in items])


@dataclass
class AIResult:
    text: str
    model_used: Optional[str] = None


class ConversationHandler:
    """High-level orchestrator for a single message turn."""

    def __init__(self, db: Session):
        self.db = db

    # -------- Public API --------
    async def process_message(
        self,
        *,
        session_id: str,
        message: str,
        channel: str = "chat",
        context: Optional[Dict[str, Any]] = None,
        language: Optional[str] = "en",
        phone_number: Optional[str] = None,
        location: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """Process a user message and return a structured response.

        Returns: {"message": str, "suggested_actions": List[Dict], "metadata": Dict}
        """
        started = _now_ms()
        ctx = context or {}
        selected_business_id: Optional[int] = ctx.get("selected_business") or ctx.get("business_id")
        user_text = (message or "").strip()

        # Optional heuristic intent just for metadata; reply will be LLM-generated
        intent = self._detect_intent(user_text)

        # Build candidates if business not fixed
        candidates: List[Tuple[Business, float]] = []
        if not selected_business_id and user_text:
            candidates = self._find_candidate_businesses(user_text)

        # LLM-first conversational response
        reply, actions, meta = self._ai_generate_response(
            session_id=session_id,
            user_text=user_text,
            selected_business_id=selected_business_id,
            candidates=candidates,
            language=language or "en",
            ctx_history=ctx.get("history"),
        )

        # Persist messages only when business is known (schema requires business_id)
        model_used = meta.get("ai_model_used")
        if selected_business_id:
            try:
                self._save_messages(
                    session_id=session_id,
                    business_id=selected_business_id,
                    user_text=user_text,
                    bot_text=reply,
                    intent=intent,
                    model_used=model_used,
                    duration_ms=_now_ms() - started,
                )
            except Exception:
                # Don't block chat on persistence failure
                pass

        return {
            "message": reply,
            "suggested_actions": actions,
            "metadata": {
                **meta,
                "intent": intent,
                "language": language,
                "business_id": selected_business_id,
            },
        }

    async def stream_response(
        self,
        *,
        session_id: str,
        message: str,
        channel: str = "chat",
        context: Optional[Dict[str, Any]] = None,
        language: Optional[str] = "en",
        phone_number: Optional[str] = None,
        location: Optional[Dict[str, Any]] = None,
        delay_ms: int = 50,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """Stream response character by character for typewriter effect.
        
        Args:
            delay_ms: Delay between characters in milliseconds (50ms = 20 chars/second)
        
        Yields:
            Dict with 'type' ('chunk'|'actions'|'metadata'|'complete') and 'data'
        """
        # Get the complete response first
        response = await self.process_message(
            session_id=session_id,
            message=message,
            channel=channel,
            context=context,
            language=language,
            phone_number=phone_number,
            location=location,
        )
        
        full_message = response["message"]
        suggested_actions = response["suggested_actions"]
        metadata = response["metadata"]
        
        # Stream the message character by character
        if full_message:
            for i, char in enumerate(full_message):
                yield {
                    "type": "chunk",
                    "data": {
                        "character": char,
                        "position": i,
                        "partial_message": full_message[:i+1]
                    }
                }
                
                # Add realistic typing delays
                if char == '\n':
                    await asyncio.sleep(delay_ms * 2 / 1000)  # Longer pause for new lines
                elif char in '.!?':
                    await asyncio.sleep(delay_ms * 3 / 1000)  # Pause after sentences
                elif char == ' ':
                    await asyncio.sleep(delay_ms * 0.5 / 1000)  # Shorter pause for spaces
                else:
                    await asyncio.sleep(delay_ms / 1000)
        
        # Send actions after message is complete
        if suggested_actions:
            yield {
                "type": "actions",
                "data": suggested_actions
            }
        
        # Send metadata
        yield {
            "type": "metadata", 
            "data": metadata
        }
        
        # Final completion signal
        yield {
            "type": "complete",
            "data": {
                "message": full_message,
                "suggested_actions": suggested_actions,
                "metadata": metadata
            }
        }

    # -------- Intent + Flows --------
    def _detect_intent(self, text: str) -> str:
        t = text.lower()
        # Very fast keyword heuristics
        if any(w in t for w in ["menu", "eat", "food", "drink", "order", "burger", "pizza", "coffee"]):
            if "order" in t or "buy" in t:
                return "order_inquiry"
            return "menu_inquiry"
        if any(w in t for w in ["book", "reserve", "reservation", "appointment", "schedule"]):
            return "reservation_inquiry"
        if any(w in t for w in ["service", "services", "what do you have", "what do u have", "offer"]):
            return "list_services"
        if any(w in t for w in ["category", "categories", "businesses"]):
            return "list_categories"
        if any(w in t for w in ["help", "hi", "hello", "start"]):
            return "greeting"
        return "general"

    def _handle_global_flow(
        self,
        user_text: str,
        intent: str,
        candidates: List[Tuple[Business, float]],
        language: Optional[str],
    ) -> Tuple[str, List[Dict[str, str]], Dict[str, Any]]:
        # Deprecated in LLM-first mode; kept for compatibility if needed elsewhere.
        actions: List[Dict[str, str]] = []
        meta: Dict[str, Any] = {"stage": "global", "deprecated": True}
        return user_text or "How can I help you today?", actions, meta

    def _handle_business_flow(
        self,
        business_id: int,
        user_text: str,
        intent: str,
        language: Optional[str],
    ) -> Tuple[str, List[Dict[str, str]], Dict[str, Any]]:
        actions: List[Dict[str, str]] = []
        meta: Dict[str, Any] = {"stage": "business"}

        business = self.db.query(Business).filter(Business.id == business_id).first()
        if not business:
            return (
                "I couldn't find that business. You can ask me to search for another.",
                actions,
                meta,
            )

        # Deprecated in LLM-first mode; kept for compatibility if needed elsewhere.
        llm = self._call_llm(
            prompt=self._build_llm_prompt(business, user_text),
            language=language or "en",
        )
        return llm.text, actions, {**meta, "ai_model_used": llm.model_used, "deprecated": True}

    # -------- DB helpers --------
    def _find_candidate_businesses(self, text: str) -> List[Tuple[Business, float]]:
        # Enhanced search across name, description, and categories
        term = text.strip().lower()
        if not term:
            # Return some popular businesses if no search term
            try:
                matches = self.db.query(Business).filter(
                    Business.is_active == True
                ).order_by(Business.id).limit(8).all()
                return [(m, 0.8) for m in matches]
            except Exception:
                return []
        
        # Multi-field search with keyword matching
        try:
            # Primary exact/partial name matches
            exact_matches = self.db.query(Business).filter(
                Business.is_active == True,
                Business.name.ilike(f"%{term}%")
            ).limit(5).all()
            
            # Description matches
            desc_matches = self.db.query(Business).filter(
                Business.is_active == True,
                Business.description.ilike(f"%{term}%")
            ).limit(5).all()
            
            # Combine results with scores
            candidates = []
            seen_ids = set()
            
            # Add exact name matches with high score
            for b in exact_matches:
                if b.id not in seen_ids:
                    candidates.append((b, 0.95))
                    seen_ids.add(b.id)
            
            # Add description matches with medium score
            for b in desc_matches:
                if b.id not in seen_ids:
                    candidates.append((b, 0.7))
                    seen_ids.add(b.id)
            
            # If still not enough results, add more businesses
            if len(candidates) < 3:
                more_matches = self.db.query(Business).filter(
                    Business.is_active == True,
                    ~Business.id.in_(seen_ids)
                ).limit(6).all()
                for b in more_matches:
                    candidates.append((b, 0.5))
            
            return candidates[:10]
            
        except Exception as e:
            # If DB isn't ready, don't crash the chat
            print(f"Business search error: {e}")  # Debug log
            return []

    def _search_menu_items(
        self, text: str, *, business_id: Optional[int], limit: int = 5
    ) -> List[MenuItem]:
        if not text:
            return []
        tokens = [t for t in re.split(r"\W+", text.lower()) if t]
        if not tokens:
            return []
        ors = []
        for tok in set(tokens):
            like = f"%{tok}%"
            ors.append(MenuItem.name.ilike(like))
            ors.append(MenuItem.description.ilike(like))
        q = self.db.query(MenuItem).filter(MenuItem.is_available == True, or_(*ors))
        if business_id:
            q = q.filter(MenuItem.business_id == business_id)
        return q.limit(limit).all()

    def _save_messages(
        self,
        *,
        session_id: str,
        business_id: int,
        user_text: str,
        bot_text: str,
        intent: str,
        model_used: Optional[str],
        duration_ms: int,
    ) -> None:
        # User message
        user_msg = Message(
            session_id=session_id,
            business_id=business_id,
            sender_type="customer",
            content=user_text,
            message_type="text",
            intent_detected=intent,
            ai_model_used=None,
            response_time_ms=None,
            extra_data={},
        )
        self.db.add(user_msg)

        # Bot message
        bot_msg = Message(
            session_id=session_id,
            business_id=business_id,
            sender_type="bot",
            content=bot_text,
            message_type="text",
            intent_detected=intent,
            ai_model_used=model_used,
            response_time_ms=duration_ms,
            extra_data={},
        )
        self.db.add(bot_msg)

        self.db.commit()

    # -------- LLM-first prompt building and parsing --------
    def _ai_generate_response(
        self,
        *,
        session_id: str,
        user_text: str,
        selected_business_id: Optional[int],
        candidates: List[Tuple[Business, float]],
        language: str,
        ctx_history: Optional[List[Dict[str, str]]] = None,
    ) -> Tuple[str, List[Dict[str, str]], Dict[str, Any]]:
        # Build lightweight context
        biz: Optional[Business] = None
        if selected_business_id:
            try:
                biz = self.db.query(Business).filter(Business.id == selected_business_id).first()
            except Exception:
                # If DB isn't ready or query fails, proceed without business context
                biz = None

        # Conversation history
        # Prefer in-memory session history from context when available; otherwise fall back to DB (last 10)
        history: List[Dict[str, str]] = []
        if ctx_history and isinstance(ctx_history, list):
            # Keep last 20 from session memory
            try:
                history = [
                    {"role": (h.get("role") or "user"), "content": h.get("content") or ""}
                    for h in ctx_history[-20:]
                    if isinstance(h, dict)
                ]
            except Exception:
                history = []
        if not history:
            try:
                # Limit by recent time window to avoid pulling in yesterday's chat
                max_age_min = int(os.getenv("CHAT_HISTORY_MAX_AGE_MINUTES", "180"))  # default 3 hours
                cutoff_dt = datetime.utcnow() - timedelta(minutes=max_age_min)
                q = self.db.query(Message).filter(
                    Message.session_id == session_id,
                    Message.created_at >= cutoff_dt,
                )
                if selected_business_id:
                    q = q.filter(Message.business_id == selected_business_id)
                q = q.order_by(Message.created_at.desc()).limit(10)
                rows = list(reversed(q.all()))
                for r in rows:
                    role = "assistant" if r.sender_type == "bot" else "user"
                    history.append({"role": role, "content": r.content or ""})
            except Exception:
                pass

        # Candidate businesses context
        cand_ctx = [
            {"id": b.id, "name": b.name, "description": b.description}
            for (b, _score) in candidates[:5]
        ]

        # Relevant items context (limit for token budget)
        items_ctx: List[Dict[str, Any]] = []
        try:
            if selected_business_id:
                items = self._search_menu_items(user_text, business_id=selected_business_id, limit=8)
                if not items:
                    items = (
                        self.db.query(MenuItem)
                        .filter(MenuItem.business_id == selected_business_id, MenuItem.is_available == True)
                        .order_by(MenuItem.display_order)
                        .limit(8)
                        .all()
                    )
            else:
                items = self._search_menu_items(user_text, business_id=None, limit=6)
            for it in items:
                items_ctx.append({
                    "id": it.id,
                    "name": it.name,
                    "price": float(it.base_price) if it.base_price is not None else None,
                    "business_id": it.business_id,
                })
        except Exception:
            pass

        # Build prompt
        prompt = self._build_conversational_prompt(
            language=language,
            user_text=user_text,
            history=history,
            business=biz,
            candidates=cand_ctx,
            items=items_ctx,
        )

        llm = self._call_llm(prompt=prompt, language=language)
        # Optional tool call phase: allow the LLM to request a tool invocation.
        tool_request = self._extract_tool_call(llm.text)
        tool_result: Optional[Dict[str, Any]] = None
        if tool_request:
            try:
                tool_result = self._run_tool(tool_request)
            except Exception as _e:  # Never fail the turn on tool errors
                tool_result = {"error": "tool_execution_failed"}

            # Second-pass prompt with tool result to get the final, user-facing answer
            second_prompt = self._build_second_pass_prompt(
                language=language,
                user_text=user_text,
                history=history,
                business=biz,
                candidates=cand_ctx,
                items=items_ctx,
                tool_request=tool_request,
                tool_result=tool_result,
            )
            llm = self._call_llm(prompt=second_prompt, language=language)

        text, actions = self._parse_llm_output(llm.text)

        # Clean and validate the response
        text = self._clean_ai_output(text)
        text = self._validate_response(text)

        # Let the LLM fully control suggested actions; do not inject defaults.

        # Minimal safety fallback if the LLM failed
        if (not text) or len(text.strip()) == 0 or ("groq llm is not configured" in text.lower()):
            text = "How can I help you today?"

        meta: Dict[str, Any] = {"ai_model_used": llm.model_used, "stage": "llm_first"}
        return text, actions, meta

    def _build_conversational_prompt(
        self,
        *,
        language: str,
        user_text: str,
        history: List[Dict[str, str]],
        business: Optional[Business],
        candidates: List[Dict[str, Any]],
        items: List[Dict[str, Any]],
    ) -> str:
        ctx: Dict[str, Any] = {
            "language": language,
            "business": (
                {"id": business.id, "name": business.name, "description": business.description}
                if business
                else None
            ),
            "candidates": candidates,
            "items": items,
            "history": history,
        }
        instructions = (
            "You are X-SevenAI, a friendly, intelligent assistant for Business Automation.\n"
            "- Always reply in the user's language: " + language + ".\n"
            "- Be warm, concise, proactive, and conversational — like ChatGPT.\n"
            "- Use Markdown for readability (headings, bold, bullet lists).\n"
            "\n"
            "CONDUCT:\n"
            "- You are in full control of the conversation — do not describe your process.\n"
            "- Decide what the user needs and respond naturally.\n"
            "- Ask targeted clarifying questions when details are missing.\n"
            "- Maintain memory with the provided history for continuity.\n"
            "\n"
            "STRICT PRIVACY OF REASONING:\n"
            "- NEVER expose your thinking, analysis, or step-by-step planning.\n"
            "- NEVER start with phrases like 'Okay', 'Let me', 'I need to', or 'First I'll'.\n"
            "- Respond ONLY with the final helpful answer.\n"
            "\n"
            "TOOL CALLING (optional):\n"
            "- If you need to search businesses, search menu items, or propose a reservation draft, request a tool by outputting a single line starting with:\n"
            "  TOOL_CALL: {\"name\":\"<tool_name>\", \"args\":{...}}\n"
            "- Available tools: search_businesses, search_menu_items, propose_appointment_draft.\n"
            "- After tools run, you will receive their JSON results and should produce the final user-facing answer without exposing the tool details.\n"
            "\n"
            "BEHAVIOR WITH CONTEXT:\n"
            "- If a business is selected, you may leverage the context (items/candidates/history) to assist.\n"
            "- If no business is selected, you may suggest one from candidates and ask to confirm.\n"
            "- You may propose next steps (e.g., view menu, start order, make reservation) based on the user's needs.\n"
            "\n"
            "OPTIONAL QUICK ACTIONS (use only when beneficial):\n"
            "Add an 'ACTIONS' section at the end with one action per line in the format:\n"
            "ACTIONS:\n- View Menu | view-menu\n- Start Order | start-order\n- Make Reservation | make-reservation\n"
        )
        return (
            instructions
            + "\n\nContext JSON:\n"
            + json.dumps(ctx, ensure_ascii=False)
            + f"\n\nUser: {user_text}\nAssistant:"
        )

    def _build_second_pass_prompt(
        self,
        *,
        language: str,
        user_text: str,
        history: List[Dict[str, str]],
        business: Optional[Business],
        candidates: List[Dict[str, Any]],
        items: List[Dict[str, Any]],
        tool_request: Dict[str, Any],
        tool_result: Dict[str, Any],
    ) -> str:
        ctx: Dict[str, Any] = {
            "language": language,
            "business": (
                {"id": business.id, "name": business.name, "description": business.description}
                if business
                else None
            ),
            "candidates": candidates,
            "items": items,
            "history": history,
            "tool_request": tool_request,
            "tool_result": tool_result,
        }
        instructions = (
            "You have received TOOL_RESULT JSON from a tool you requested.\n"
            "- Use it to answer the user.\n"
            "- Do NOT show tool details or internal steps.\n"
            "- Respond only with the final helpful answer in Markdown, optionally with ACTIONS as previously described.\n"
        )
        return (
            instructions
            + "\n\nContext JSON:\n"
            + json.dumps(ctx, ensure_ascii=False)
            + f"\n\nUser: {user_text}\nAssistant:"
        )

    def _extract_tool_call(self, text: str) -> Optional[Dict[str, Any]]:
        """Parse a tool call instruction of the form:
        TOOL_CALL: {"name":"<tool>", "args":{...}}
        Returns a dict {name:str, args:dict} or None.
        """
        if not text:
            return None
        m = re.search(r"^\s*TOOL_CALL:\s*(\{[\s\S]*\})\s*$", text, flags=re.IGNORECASE | re.MULTILINE)
        if not m:
            return None
        try:
            obj = json.loads(m.group(1))
            name = (obj.get("name") or "").strip()
            args = obj.get("args") or {}
            if not name or not isinstance(args, dict):
                return None
            return {"name": name, "args": args}
        except Exception:
            return None

    def _run_tool(self, req: Dict[str, Any]) -> Dict[str, Any]:
        """Execute supported tools (read-only / draft) and return JSON results."""
        name = (req.get("name") or "").strip()
        args = req.get("args") or {}

        if name == "search_businesses":
            query = str(args.get("query") or "").strip()
            cand = self._find_candidate_businesses(query)
            return {
                "tool": name,
                "query": query,
                "results": [
                    {"id": b.id, "name": b.name, "description": b.description}
                    for (b, _s) in cand[:10]
                ],
            }

        if name == "search_menu_items":
            query = str(args.get("query") or "").strip()
            business_id = args.get("business_id")
            try:
                business_id = int(business_id) if business_id is not None else None
            except Exception:
                business_id = None
            items = self._search_menu_items(query, business_id=business_id, limit=8)
            return {
                "tool": name,
                "query": query,
                "business_id": business_id,
                "results": [
                    {
                        "id": it.id,
                        "name": it.name,
                        "price": float(it.base_price) if it.base_price is not None else None,
                        "business_id": it.business_id,
                    }
                    for it in items
                ],
            }

        if name == "propose_appointment_draft":
            # Validate minimal fields and return a draft; no DB writes to keep it safe.
            draft: Dict[str, Any] = {
                "business_id": args.get("business_id"),
                "service_name": args.get("service_name"),
                "scheduled_date": args.get("scheduled_date"),
                "start_time": args.get("start_time"),
                "end_time": args.get("end_time"),
                "duration_minutes": args.get("duration_minutes"),
                "customer_name": args.get("customer_name"),
                "customer_phone": args.get("customer_phone"),
            }
            missing = [k for k, v in draft.items() if v in (None, "")]
            return {
                "tool": name,
                "draft": draft,
                "missing_fields": missing,
                "requires_confirmation": True,
            }

        return {"tool": name or "unknown", "error": "unsupported_tool"}

    def _parse_llm_output(self, text: str) -> Tuple[str, List[Dict[str, str]]]:
        actions: List[Dict[str, str]] = []
        if not text:
            return "", actions

        body = text.replace("\r\n", "\n")
        # Remove any explicit chain-of-thought or internal leakage before parsing actions
        body = self._clean_ai_output(body)

        # Find the ACTIONS header case-insensitively with optional spaces
        m = re.search(r"\n\s*actions\s*:\s*\n", body, flags=re.IGNORECASE)
        if not m:
            return body.strip(), actions

        main = body[: m.start()].strip()
        tail = body[m.end() :]

        # Collect lines belonging to the ACTIONS block
        lines: List[str] = []
        for line in tail.splitlines():
            # Stop if another SECTION HEADER appears (e.g., NEXT:, NOTES:)
            if re.match(r"^\s*[A-Z][A-Z _-]{2,}:\s*$", line):
                break
            # Ignore code-fence markers
            if re.match(r"^\s*```", line):
                continue
            # Keep the line (even if blank, we'll filter later)
            lines.append(line)

        # Normalize and parse potential action lines
        for raw in lines:
            s = raw.strip()
            if not s:
                continue
            # Strip common bullet/numbering prefixes
            s = s.lstrip(" -*•\t")
            s = re.sub(r"^\d+[\.)]\s*", "", s)

            if "|" in s:
                title, action_id = [x.strip() for x in s.split("|", 1)]
                if title and action_id:
                    actions.append({"id": action_id, "title": title})

        return main, actions

    def _strip_thinking(self, s: str) -> str:
        """Backward-compatible basic cleaner (kept for compatibility)."""
        lines: List[str] = []
        for line in s.splitlines():
            if re.match(r"^\s*(thinking|reasoning|analysis|chain[- ]?of[- ]?thoughts?)\s*:\s*", line, flags=re.IGNORECASE):
                continue
            lines.append(line)
        return "\n".join(lines)

    def _clean_ai_output(self, s: str) -> str:
        """Remove ALL internal reasoning, meta-commentary, and debug content."""
        if not s:
            return s
            
        text = s.strip()
        
        # Remove XML-like blocks
        text = re.sub(r"<\s*(thinking|analysis|internal|reasoning|debug|chain.*thought)[^>]*>.*?<\s*/\s*\1\s*>", "", text, flags=re.IGNORECASE | re.DOTALL)
        
        # Remove fenced code blocks with thinking labels
        text = re.sub(r"```(?:\s*(thinking|analysis|reasoning|debug|chain[- ]?of[- ]?thought))\b[\s\S]*?```", "", text, flags=re.IGNORECASE)
        
        # Split into lines and filter aggressively
        lines = text.split('\n')
        cleaned_lines = []
        skip_next_lines = 0
        
        for i, line in enumerate(lines):
            if skip_next_lines > 0:
                skip_next_lines -= 1
                continue
                
            line_lower = line.lower().strip()
            
            # Skip lines that start with problematic phrases
            problematic_starts = [
                'okay, the user',
                'let me check',
                'i need to',
                'first, i',
                'let me',
                'i should',
                'i will',
                'analyzing',
                'thinking',
                'reasoning',
                'debug',
                'internal',
                'looking at',
                'based on the context',
                'checking the',
                'since the user',
                'the user wants',
                'the user is asking',
                'i can see',
                'i notice',
                'examining the'
            ]
            
            if any(line_lower.startswith(phrase) for phrase in problematic_starts):
                # Skip this line and potentially the next few if it looks like a reasoning block
                if i < len(lines) - 1 and not lines[i + 1].strip():
                    skip_next_lines = 1  # Skip empty line after reasoning
                continue
                
            # Skip lines that are clearly meta-commentary
            if re.match(r'^\s*(step \d+|first|second|third|next|then|finally)\s*[:.-]', line_lower):
                continue
                
            # Skip empty lines that follow removed reasoning
            if not line.strip() and cleaned_lines and not cleaned_lines[-1].strip():
                continue
                
            cleaned_lines.append(line)
        
        # Join and clean up extra whitespace
        result = '\n'.join(cleaned_lines).strip()
        
        # Remove multiple consecutive empty lines
        result = re.sub(r'\n\s*\n\s*\n+', '\n\n', result)
        
        return result

    def _validate_response(self, text: str) -> str:
        """Ensure response doesn't contain internal reasoning."""
        if not text:
            return "How can I help you today?"
            
        # Check for reasoning leakage
        problematic_phrases = [
            'okay, the user',
            'let me check',
            'i need to',
            'analyzing the',
            'looking at the context',
            'since no business is selected',
            'the user wants to'
        ]
        
        text_lower = text.lower()
        if any(phrase in text_lower for phrase in problematic_phrases):
            # If reasoning detected, return a generic helpful response
            return "I'm here to help! What would you like to know about our services?"
        
        return text

    # -------- LLM integration (Groq-first) --------
    def _build_llm_prompt(self, business: Business, user_text: str) -> str:
        details = [
            f"Business: {business.name}",
            f"Description: {business.description or 'N/A'}",
        ]
        return (
            "You are a helpful assistant for a local business. Answer clearly in markdown.\n"
            "Focus on being concise, actionable, and polite. Make conversation\n\n" + "\n".join(details) +
            f"\n\nUser: {user_text}\nAssistant:"
        )

    def _call_llm(self, *, prompt: str, language: str) -> AIResult:
        # Groq-only per product direction; surface a clear message if key missing
        # Prefer value from settings (loaded from .env), fallback to environment
        groq_key = settings.GROQ_API_KEY or os.getenv("GROQ_API_KEY")
        if not groq_key:
            return AIResult(
                text=(
                    "Groq LLM is not configured. Set GROQ_API_KEY in your environment to enable conversational AI."
                ),
                model_used=None,
            )
        # Select model (configurable via settings.GROQ_MODEL or env GROQ_MODEL)
        # Use a widely-available default and prepare fallbacks if the configured model isn't available.
        configured_model = getattr(settings, "GROQ_MODEL", None) or os.getenv("GROQ_MODEL")
        default_model = "qwen/qwen3-32b"
        fallback_models = [
            default_model,
            #"qwen-2.5-32b",
            "mixtral-8x7b-32768",
        ]
        # Start with the configured model (if any), then try fallbacks
        model_try_order = []
        if configured_model:
            model_try_order.append(configured_model)
        for m in fallback_models:
            if m not in model_try_order:
                model_try_order.append(m)
        try:
            # Prefer httpx if available; fall back to urllib to avoid SDK version issues
            try:
                import httpx  # type: ignore

                url = "https://api.groq.com/openai/v1/chat/completions"
                headers = {
                    "Authorization": f"Bearer {groq_key}",
                    "Content-Type": "application/json",
                }
                last_err: Optional[Exception] = None
                with httpx.Client(timeout=15.0) as client:
                    for model_name in model_try_order:
                        payload = {
                            "model": model_name,
                            "messages": [
                                {"role": "system", "content": "Reply in markdown. Be friendly, natural and concise. NEVER show internal reasoning or thinking process."},
                                {"role": "user", "content": prompt},
                            ],
                            "temperature": 0.4,
                        }
                        try:
                            resp = client.post(url, headers=headers, json=payload)
                            resp.raise_for_status()
                            data = resp.json()
                            text = data["choices"][0]["message"]["content"]
                            return AIResult(text=text, model_used=f"groq:{model_name}")
                        except httpx.HTTPStatusError as he:  # type: ignore
                            # Model not found or not available typically returns 404 from OpenAI-compatible APIs
                            if he.response is not None and he.response.status_code in (400, 404):
                                # Try next fallback
                                last_err = he
                                continue
                            last_err = he
                            break
                        except Exception as e:
                            last_err = e
                            break
                # If we reach here, all attempts failed
                raise last_err or RuntimeError("Failed to complete request to Groq API")
            except ModuleNotFoundError:
                import json as _json
                from urllib.request import Request, urlopen  # type: ignore

                url = "https://api.groq.com/openai/v1/chat/completions"
                last_err: Optional[Exception] = None
                for model_name in model_try_order:
                    payload = _json.dumps({
                        "model": model_name,
                        "messages": [
                            {"role": "system", "content": "Reply in markdown. Be friendly, natural and concise. NEVER show internal reasoning or thinking process."},
                            {"role": "user", "content": prompt},
                        ],
                        "temperature": 0.4,
                    }).encode("utf-8")
                    req = Request(url, data=payload, headers={
                        "Authorization": f"Bearer {groq_key}",
                        "Content-Type": "application/json",
                    })
                    try:
                        with urlopen(req, timeout=15) as resp:  # nosec - trusted API endpoint
                            data = _json.loads(resp.read().decode("utf-8"))
                        text = data["choices"][0]["message"]["content"]
                        return AIResult(text=text, model_used=f"groq:{model_name}")
                    except Exception as e:
                        last_err = e
                        continue
                raise last_err or RuntimeError("Failed to complete request to Groq API")
        except Exception as e:
            return AIResult(text=f"LLM error: {e}", model_used=f"groq:{model_name}")